<!DOCTYPE html>
<html lang="en">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Smart Assistant</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="Orquestrating Cognitive LLM Agents
LLM-based multi-agent systems still face several key challenges that impact their effectiveness and scalability. One of the main challenges is ensuring seamless communication and coordination among agents, as these systems often involve complex interactions across various tasks and domains. We have developed robust mechanisms for synchronization and negotiation, enabling LLM conversational agents to understand and respond to each other’s actions in real time. We propose a novel multi-agent system architecture where LLM-powered cognitive agents interact to analyze data effectively. Each agent within the system is designed to perform specialized roles, such as communication, coordination, and code generation, enabling task distribution and cooperative problem-solving. Unlike monolithic AI systems, our proposal promotes modularity and scalability, ensuring that individual agents can evolve independently while collaborating with other agents.">
    <meta name="generator" content="Hugo 0.152.2">
    
    
    
      <meta name="robots" content="noindex, nofollow">
    
    

    
<link rel="stylesheet" href="/ananke/css/main.min.css" >




    


    
      
<link rel="shortcut icon" href="/img/favicon.ico" type="image/x-icon" />


    

    

    
      <link rel="canonical" href="//localhost:1313/research/">
    

    
    
    <meta property="og:url" content="//localhost:1313/research/">
  <meta property="og:site_name" content="Smart Assistant">
  <meta property="og:title" content="Smart Assistant">
  <meta property="og:description" content="Orquestrating Cognitive LLM Agents LLM-based multi-agent systems still face several key challenges that impact their effectiveness and scalability. One of the main challenges is ensuring seamless communication and coordination among agents, as these systems often involve complex interactions across various tasks and domains. We have developed robust mechanisms for synchronization and negotiation, enabling LLM conversational agents to understand and respond to each other’s actions in real time. We propose a novel multi-agent system architecture where LLM-powered cognitive agents interact to analyze data effectively. Each agent within the system is designed to perform specialized roles, such as communication, coordination, and code generation, enabling task distribution and cooperative problem-solving. Unlike monolithic AI systems, our proposal promotes modularity and scalability, ensuring that individual agents can evolve independently while collaborating with other agents.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">

  <meta itemprop="name" content="Smart Assistant">
  <meta itemprop="description" content="Orquestrating Cognitive LLM Agents LLM-based multi-agent systems still face several key challenges that impact their effectiveness and scalability. One of the main challenges is ensuring seamless communication and coordination among agents, as these systems often involve complex interactions across various tasks and domains. We have developed robust mechanisms for synchronization and negotiation, enabling LLM conversational agents to understand and respond to each other’s actions in real time. We propose a novel multi-agent system architecture where LLM-powered cognitive agents interact to analyze data effectively. Each agent within the system is designed to perform specialized roles, such as communication, coordination, and code generation, enabling task distribution and cooperative problem-solving. Unlike monolithic AI systems, our proposal promotes modularity and scalability, ensuring that individual agents can evolve independently while collaborating with other agents.">
  <meta itemprop="wordCount" content="856">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Smart Assistant">
  <meta name="twitter:description" content="Orquestrating Cognitive LLM Agents LLM-based multi-agent systems still face several key challenges that impact their effectiveness and scalability. One of the main challenges is ensuring seamless communication and coordination among agents, as these systems often involve complex interactions across various tasks and domains. We have developed robust mechanisms for synchronization and negotiation, enabling LLM conversational agents to understand and respond to each other’s actions in real time. We propose a novel multi-agent system architecture where LLM-powered cognitive agents interact to analyze data effectively. Each agent within the system is designed to perform specialized roles, such as communication, coordination, and code generation, enabling task distribution and cooperative problem-solving. Unlike monolithic AI systems, our proposal promotes modularity and scalability, ensuring that individual agents can evolve independently while collaborating with other agents.">

	
  </head><body class="ma0 avenir bg-near-white development">

    
   
  

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l center items-center justify-between">
    <a href="/about" class="f3 fw2 hover-white white-90 dib no-underline">
      
        Smart Assistant
      
    </a>
    <div class="flex-l items-center">
      

      
        <ul class="pl0 mr3">
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white white-90 no-underline" href="/about" title="Home page">
              Home
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white white-90 no-underline" href="/research" title="Research page">
              Research
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white white-90 no-underline" href="/members/" title="Members page">
              Members
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white white-90 no-underline" href="/awards" title="Awards page">
              Awards
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white white-90 no-underline" href="/publications" title="Publications page">
              Publications
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white white-90 no-underline" href="/products" title="Products page">
              Products
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white white-90 no-underline" href="/contact" title="Contact page">
              Contact
            </a>
          </li>
          
        </ul>
      
      <div class="ananke-socials"></div>

    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l mw7 center ph3 flex-wrap justify-between">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked ttu">
          
        
      </aside><div id="sharing" class="mt3 ananke-socials"></div>
<h1 class="f1 athelas mt3 mb1"></h1>
      
      
      

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-100-l"><h1 id="orquestrating-cognitive-llm-agents">Orquestrating Cognitive LLM Agents</h1>
<p>LLM-based multi-agent systems still face several key challenges that impact their effectiveness and scalability. One of the main challenges is ensuring seamless communication and coordination among agents, as these systems often involve complex interactions across various tasks and domains. We have developed robust mechanisms for synchronization and negotiation, enabling LLM conversational agents to understand and respond to each other’s actions in real time. We propose a novel multi-agent system architecture where LLM-powered cognitive agents interact to analyze data effectively. Each agent within the system is designed to perform specialized roles, such as communication, coordination, and code generation, enabling task distribution and cooperative problem-solving. Unlike monolithic AI systems, our proposal promotes modularity and scalability, ensuring that individual agents can evolve independently while collaborating with other agents.</p>
<h1 id="planning-in-llm-based-cognitive-multiagent-systems">Planning in LLM-based cognitive Multiagent Systems</h1>
<p>Plan generation refers to selecting actions at a given moment from a set of possible actions within a defined environment and objective, requiring agents not only to perceive environmental conditions but also to account for other agents&rsquo; plans. We have developed novel planning approaches to anticipate and adapt to unexpected environmental changes, particularly in dynamic and data-centric settings. These strategies encompass both deriving plans for task-driven agents, leveraging different strategies towards a specific goal, and designing orchestration paradigms in which one agent must best organize a group of distinct task-driven agents. Efficient plan generation is closely tied to Memory Systems, particularly when using external modules to access plan guidelines elaborated by human specialists to aid the agent in predetermined scenarios. This approach can be extended to optimize these guidelines based on the plan&rsquo;s success or failure, thereby supporting the co-creation of plans with humans and LLM agents.</p>
<h1 id="memory-in-llm-agents">Memory in LLM Agents</h1>
<p>LLM agents increasingly incorporate external memory to support long-term reasoning, adaptation, and task generalization. Prior research has examined reflective memory for self-evaluation and semantic memory for storing abstracted knowledge, but the interaction between these components has received limited attention. We introduce a unified memory architecture for LLM agents in which reflection and semantic memory co-evolve to improve agents&rsquo; lifelong learning capabilities. Our approach leverages reflection as a signal for curation of semantic memory consolidation, enabling selective strengthening, stabilization, and forgetting of stored long-term knowledge. Conversely, we propose a semantic-augmented reflection mechanism in which retrieved semantic knowledge enriches the agent&rsquo;s self-critique, yielding more profound, more transferable insights. Our project further investigates approaches to forgetting in episodic memory and how to integrate distinct memory types via working memory mechanisms.</p>
<h1 id="language-model-refinement-and-retrieval-augmented-generation">Language Model Refinement and Retrieval-Augmented Generation</h1>
<p>Language model refinement focuses on adapting language models to specific domains or tasks through fine-tuning, enabling them to better capture domain-specific terminology, structures, and reasoning patterns. Fine-tuning can be applied both to embedding models, improving information retrieval by learning representations better aligned with domain-specific semantics, and to generative models, specializing them for particular tasks or areas of expertise by incorporating curated knowledge directly into the model parameters. A core objective of our work is to improve information retrieval quality, as retrieval performance directly impacts downstream reasoning and generation.</p>
<p>Complementarily, Retrieval-Augmented Generation (RAG) integrates language models with external knowledge sources to ground generation in retrieved evidence, significantly reducing hallucinations and improving factual consistency. This paradigm is particularly effective in domains that are specialized, proprietary, or insufficiently covered during pretraining. Our research explores the refinement of individual RAG components, including retrievers, and generators, and their joint optimization to strengthen the overall system synergy and end-to-end performance.</p>
<p>In addition to fine-tuning–based approaches, we also investigate retrieval enhancement methods that do not require model fine-tuning, such as LLM-driven query expansion and reformulation. These techniques aim to bridge lexical and semantic gaps between user queries and documents, improving recall and robustness in rapidly evolving domains. By combining fine-tuned and non–fine-tuned strategies, our approach provides flexible and scalable solutions for high-performance RAG systems in domain-specific settings beyond general knowledge.</p>
<h1 id="systematic-evaluation-of-llm-based-multi-agent-systems">Systematic Evaluation of LLM-based Multi-agent Systems</h1>
<p>The rapid evolution of LLM-based Multi-Agent Systems (MAS) has intensified the need for standardized assessment protocols, as current practices frequently lack the methodological rigor required to evaluate complex tasks. This research addresses this gap by designing and developing a comprehensive technical framework for assessing LLM-based MAS. We have established a comprehensive taxonomy that categorizes evaluation methods, metrics, and specialized tools. By differentiating between individual-agent competencies, such as reasoning and tool use, and collective emergent behaviors, such as coordination and communication efficiency, our study identifies the essential requirements for a robust evaluation framework. Our approach integrates Ethical, Legal, and Social Aspects (ELSA) to ensure that technical effectiveness is balanced with responsible AI principles. This approach moves beyond traditional “LLM-as-a-Judge” paradigms, providing a reproducible, domain-agnostic standard that enhances observability and traceability across diverse agentic ecosystems.</p>
<h1 id="oil--gas-benchmark-construction">Oil &amp; Gas Benchmark Construction</h1>
<p>Development of structured benchmarks and datasets to evaluate and calibrate the performance of large language models (LLMs) and multi-agent systems, as well as ontology engineering. This initiative focuses on the oil and gas sector, specifically on Floating Production Storage and Offloading (FPSO) units. The process aims to construct a representative set of real-world scenarios, operational decisions, and technical contexts that reflect the complexity of activities conducted on offshore production facilities.</p>
<ul class="pa0">
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div></article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white white-70 dn dib-ns pv2 ph3 no-underline" href="//localhost:1313/" >
    &copy;  Smart Assistant 2025 
  </a>
    <div><div class="ananke-socials"></div>
</div>
  </div>
</footer>

  </body>
</html>
